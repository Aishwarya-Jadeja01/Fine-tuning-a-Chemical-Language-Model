{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a03209e8-b7c4-47bd-b483-0de4607a100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jadej\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from datasets import Dataset\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83905509-b16c-40d3-bd4a-17a474836cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and model paths\n",
    "DATASET_PATH = \"scikit-fingerprints/MoleculeNet_Lipophilicity\"\n",
    "MODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"\n",
    "EXTERNAL_DATASET_PATH = \"C:/Users/jadej/OneDrive/Desktop/Studies/NNTI 11th Feb/Project/Project_Files/tasks/External-Dataset_for_Task2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208a7d65-cc67-4709-8cbb-ac5a7071b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b45a39-0b65-4392-a124-d71427e084d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from HuggingFace\n",
    "dataset = load_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f2b0f1-11bf-4982-a82e-727cec03328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max SMILES length: 267\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum length of SMILES strings in the dataset\n",
    "max_length = max(len(smile) for smile in dataset['train']['SMILES'])\n",
    "print(f\"Max SMILES length: {max_length}\")\n",
    "\n",
    "\n",
    "\n",
    "# define a PyTorch Dataset class for handling SMILES strings and targets\n",
    "\n",
    "# TODO: your code goes here\n",
    "# class SMILESDataset(Dataset):\n",
    "\n",
    "\n",
    "class SMILEDATASET(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.rename(columns={'label': 'Label'})  # Standardize key names\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        return {\n",
    "            'SMILES': sample['SMILES'],\n",
    "            'Label': sample['Label']  # Ensure correct key mapping\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a2b14d-8a4f-4e93-bf25-73543187c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external dataset\n",
    "ext_data = pd.read_csv(EXTERNAL_DATASET_PATH)\n",
    "# Load the pre-trained model and tokenizer\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# ext_data = ext_data1.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13316cb-79a3-43d3-9b10-32417dee6e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCCN1Cc2c(nc3cc(-c4ccco4)nn3c2O)C1=O</td>\n",
       "      <td>1.548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cc1cc(C)c2c(n1)sc1c2ncnc1N1CCN(C)CC1</td>\n",
       "      <td>2.568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COC(=O)[C@H]1[C@H]2CC[C@H](C[C@@H]1OC(=O)c1ccc...</td>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nc1nonc1/C(=N/O)Nc1ccc(F)c(Cl)c1</td>\n",
       "      <td>2.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cc1c[nH]c(/C=C2/C(=O)Nc3ccccc32)c1CCC(=O)O</td>\n",
       "      <td>1.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              SMILES  Label\n",
       "0              CCCCN1Cc2c(nc3cc(-c4ccco4)nn3c2O)C1=O  1.548\n",
       "1               Cc1cc(C)c2c(n1)sc1c2ncnc1N1CCN(C)CC1  2.568\n",
       "2  COC(=O)[C@H]1[C@H]2CC[C@H](C[C@@H]1OC(=O)c1ccc...  0.102\n",
       "3                   Nc1nonc1/C(=N/O)Nc1ccc(F)c(Cl)c1  2.450\n",
       "4         Cc1c[nH]c(/C=C2/C(=O)Nc3ccccc32)c1CCC(=O)O  1.040"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "970d709f-3549-4a80-9a55-1fc9eb20447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MolformerForSequenceClassification(\n",
       "  (molformer): MolformerModel(\n",
       "    (embeddings): MolformerEmbeddings(\n",
       "      (word_embeddings): Embedding(2362, 768, padding_idx=2)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (encoder): MolformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x MolformerLayer(\n",
       "          (attention): MolformerAttention(\n",
       "            (self): MolformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (rotary_embeddings): MolformerRotaryEmbedding()\n",
       "              (feature_map): MolformerFeatureMap(\n",
       "                (kernel): ReLU()\n",
       "              )\n",
       "            )\n",
       "            (output): MolformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): MolformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MolformerOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): MolformerClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dense2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (classifier_act_fn): GELUActivation()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, trust_remote_code=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d4cc34-9b41-4ce6-bd44-90be486ddefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model, smiles, targets):\n",
    "    \"\"\"Compute gradients of loss with respect to model parameters.\"\"\"\n",
    "    model.zero_grad()\n",
    "\n",
    "    # ðŸ”¹ Ensure smiles is a string, not a tensor/dict\n",
    "    if isinstance(smiles, torch.Tensor):\n",
    "        smiles = smiles.item()  # Convert single tensor to string\n",
    "    elif isinstance(smiles, list) and isinstance(smiles[0], torch.Tensor):\n",
    "        smiles = [s.item() for s in smiles]  # Convert list of tensors to strings\n",
    "    \n",
    "    inputs = smiles\n",
    "    # ðŸ”¹ Ensure targets are in tensor format\n",
    "    # targets = torch.tensor([targets], dtype=torch.float32)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    \n",
    "    # ðŸ”¹ Forward pass\n",
    "    # outputs = model(**inputs)\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "\n",
    "    # ðŸ”¹ Compute loss\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    loss = loss_fn(outputs.logits.squeeze(), targets)\n",
    "\n",
    "    # ðŸ”¹ Compute gradients\n",
    "    loss.backward()\n",
    "    return [p.grad for p in model.parameters() if p.grad is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac979099-4f19-4867-bb04-2917fc282af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lissa_approximation(model, test_grad, train_data, iters=100, damping=0.01):\n",
    "    ihvp = [torch.zeros_like(tg) for tg in test_grad]\n",
    "\n",
    "    for _ in range(iters):\n",
    "        random_idx = random.randint(0, len(train_data) - 1)\n",
    "        train_sample = train_data[random_idx]  # Get one sample\n",
    "\n",
    "        # Debug print to check data\n",
    "        # print(f\"Sample retrieved: {train_sample}\")\n",
    "\n",
    "        smiles_input = train_sample.get('SMILES', None)\n",
    "        label_value = train_sample.get('label', None)  # Change from 'Label' to 'label'\n",
    "        # print(\"smiles lissa \", smiles_input)\n",
    "        inputs = tokenizer(train_sample['SMILES'], return_tensors=\"pt\")\n",
    "        label_value = torch.tensor([train_sample['label']])\n",
    "        # Safety check: If label or SMILES is missing, raise an error\n",
    "        if smiles_input is None or label_value is None:\n",
    "            raise KeyError(f\"Missing data in sample: {train_sample}\")\n",
    "\n",
    "        train_grad = compute_gradients(model, inputs, label_value)\n",
    "        ihvp = [g - damping * ih for g, ih in zip(train_grad, ihvp)]\n",
    "    \n",
    "    return ihvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e302e9d3-5ff1-466e-a7cd-749d40eb4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_influence_scores(model, external_data, train_data):\n",
    "    \"\"\"Compute influence scores for each external data point.\"\"\"\n",
    "    influence_scores = []\n",
    "    for _, row in tqdm(external_data.iterrows(), total=len(external_data)):\n",
    "        # Tokenize input molecule SMILES string\n",
    "        inputs = tokenizer(row['SMILES'], return_tensors=\"pt\")\n",
    "        target = torch.tensor([row['Label']])\n",
    "        # print(\"done\")\n",
    "        \n",
    "        # Compute test gradient\n",
    "        test_grad = compute_gradients(model, inputs, target)\n",
    "\n",
    "        # Approximate inverse Hessian-vector product\n",
    "        ihvp = lissa_approximation(model, test_grad, train_data)\n",
    "\n",
    "        # Compute influence score (dot product of test gradient and ihvp)\n",
    "        influence = sum(torch.dot(tg.flatten(), ih.flatten()) for tg, ih in zip(test_grad, ihvp))\n",
    "        influence_scores.append((row['SMILES'], row['Label'], influence.item()))\n",
    "\n",
    "    return sorted(influence_scores, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ac39ca7-23ac-4e1d-9ec0-ff502ff189cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training split of the Lipophilicity dataset (SMILEDATASET)\n",
    "SMILEDATASET = load_dataset(\"scikit-fingerprints/MoleculeNet_Lipophilicity\", split=\"train\")\n",
    "\n",
    "# Assign SMILEDATASET to train_data\n",
    "train_data = SMILEDATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96c0ae02-1579-440c-9c43-2e7492e1d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/300 [00:00<?, ?it/s]C:\\Users\\jadej\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|                                                                                          | 0/300 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compute influence scores\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m influence_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_influence_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Select top-48 influential samples\u001b[39;00m\n\u001b[0;32m      5\u001b[0m selected_samples \u001b[38;5;241m=\u001b[39m influence_scores[:\u001b[38;5;241m48\u001b[39m]\n",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m, in \u001b[0;36mcompute_influence_scores\u001b[1;34m(model, external_data, train_data)\u001b[0m\n\u001b[0;32m     11\u001b[0m test_grad \u001b[38;5;241m=\u001b[39m compute_gradients(model, inputs, target)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Approximate inverse Hessian-vector product\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m ihvp \u001b[38;5;241m=\u001b[39m \u001b[43mlissa_approximation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Compute influence score (dot product of test gradient and ihvp)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m influence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(torch\u001b[38;5;241m.\u001b[39mdot(tg\u001b[38;5;241m.\u001b[39mflatten(), ih\u001b[38;5;241m.\u001b[39mflatten()) \u001b[38;5;28;01mfor\u001b[39;00m tg, ih \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(test_grad, ihvp))\n",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m, in \u001b[0;36mlissa_approximation\u001b[1;34m(model, test_grad, train_data, iters, damping)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m smiles_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m label_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing data in sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_sample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     train_grad \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     ihvp \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;241m-\u001b[39m damping \u001b[38;5;241m*\u001b[39m ih \u001b[38;5;28;01mfor\u001b[39;00m g, ih \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_grad, ihvp)]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ihvp\n",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[1;34m(model, smiles, targets)\u001b[0m\n\u001b[0;32m     16\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# ðŸ”¹ Forward pass\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# outputs = model(**inputs)\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# ðŸ”¹ Compute loss\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\ibm\\MoLFormer-XL-both-10pct\\7b12d946c181a37f6012b9dc3b002275de070314\\modeling_molformer.py:873\u001b[0m, in \u001b[0;36mMolformerForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    871\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 873\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmolformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    885\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\ibm\\MoLFormer-XL-both-10pct\\7b12d946c181a37f6012b9dc3b002275de070314\\modeling_molformer.py:682\u001b[0m, in \u001b[0;36mMolformerModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    678\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    680\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids\u001b[38;5;241m=\u001b[39minput_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds)\n\u001b[1;32m--> 682\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    692\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(sequence_output)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\ibm\\MoLFormer-XL-both-10pct\\7b12d946c181a37f6012b9dc3b002275de070314\\modeling_molformer.py:426\u001b[0m, in \u001b[0;36mMolformerEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    418\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    419\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    420\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m         layer_head_mask,\n\u001b[0;32m    424\u001b[0m     )\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 426\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\ibm\\MoLFormer-XL-both-10pct\\7b12d946c181a37f6012b9dc3b002275de070314\\modeling_molformer.py:371\u001b[0m, in \u001b[0;36mMolformerLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    368\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    369\u001b[0m outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:261\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\ibm\\MoLFormer-XL-both-10pct\\7b12d946c181a37f6012b9dc3b002275de070314\\modeling_molformer.py:380\u001b[0m, in \u001b[0;36mMolformerLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    379\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 380\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\ibm\\MoLFormer-XL-both-10pct\\7b12d946c181a37f6012b9dc3b002275de070314\\modeling_molformer.py:338\u001b[0m, in \u001b[0;36mMolformerOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 338\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    340\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute influence scores\n",
    "influence_scores = compute_influence_scores(model, ext_data, train_data)\n",
    "\n",
    "# Select top-48 influential samples\n",
    "selected_samples = influence_scores[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00409ec6-8eba-4fe7-ab3c-8628ed431943",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df = pd.DataFrame(influence_scores, columns=[\"Smiles\", \"Label\", \"Influence\"])\n",
    "inf_df.to_csv(\"Influence.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a287fd5-a04a-41ea-98d0-ede358da2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(influence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9118246-78ca-4f7f-a93f-d9f438a8bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "788083d3-cd3d-4445-9625-2d9367c8fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "influence_df = pd.read_csv(\"Influence.csv\")  # Ensure the correct file path\n",
    "\n",
    "# Select the top 100 SMILES strings and corresponding Labels based on Influence score\n",
    "top_100 = influence_df.nlargest(100, 'Influence')[[\"Smiles\", \"Label\"]]  # Extract only required columns\n",
    "\n",
    "# Convert to lists\n",
    "top_100_smiles = top_100[\"Smiles\"].tolist()\n",
    "top_100_labels = top_100[\"Label\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "class TopInfluenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, smiles_list, labels_list, tokenizer, max_length=268):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.labels_list = labels_list  # Store labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length  # Set max length for consistent padding\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        label = torch.tensor(self.labels_list[idx], dtype=torch.float32)  # Convert label to tensor\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            smiles,\n",
    "            padding=\"max_length\",  # Ensure all sequences have the same length\n",
    "            truncation=True,  # Truncate if sequence exceeds max_length\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        tokenized_data = {key: val.squeeze(0) for key, val in encoded.items()}  # Remove batch dim\n",
    "        tokenized_data[\"label\"] = label  # Add label to dictionary\n",
    "\n",
    "        return tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "\n",
    "# Create dataset instance\n",
    "top_100_data_loader = torch.utils.data.DataLoader(\n",
    "    TopInfluenceDataset(top_100_smiles, top_100_labels, tokenizer),  # Pass only the SMILES list\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    # collate_fn=collate_fn  # Ensure proper batching\n",
    ")\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "# Split train_data into train (80%) and test (20%) sets\n",
    "train_size = int(0.8 * len(train_data))\n",
    "test_size = len(train_data) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(train_data, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for train and test data\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "# print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}, Top 100 dataset: {len(top_100_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2d00cfa-c09c-41f3-ab37-01779749b73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TopInfluenceDataset at 0x1bc166cf8c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5adca28f-5b71-4a28-abfb-8f36797eb16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BB33753320>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-Tuning Regression (MLM) Epoch 1/7:   0%|                                | 0/7 [00:07<?, ?it/s, Training Loss=1.07]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 1/7:   0%|                               | 0/7 [00:13<?, ?it/s, Training Loss=0.821]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 1/7:   0%|                               | 0/7 [00:20<?, ?it/s, Training Loss=0.404]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 1/7:   0%|                               | 0/7 [00:27<?, ?it/s, Training Loss=0.258]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 1/7:   0%|                               | 0/7 [00:34<?, ?it/s, Training Loss=0.567]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 1/7:   0%|                                | 0/7 [00:41<?, ?it/s, Training Loss=0.36]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 1/7:   0%|                               | 0/7 [00:43<?, ?it/s, Training Loss=0.588]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7, Average Loss: 0.5815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Regression (MLM) Epoch 2/7:   0%|                                                    | 0/7 [00:00<?, ?it/s]\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BB33753320>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Regression (MLM) Epoch 2/7:   0%|                               | 0/7 [00:43<?, ?it/s, Training Loss=0.323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7, Average Loss: 0.3182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BB33753320>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-Tuning Regression (MLM) Epoch 3/7:   0%|                               | 0/7 [00:06<?, ?it/s, Training Loss=0.117]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 3/7:   0%|                               | 0/7 [00:13<?, ?it/s, Training Loss=0.424]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 3/7:   0%|                               | 0/7 [00:20<?, ?it/s, Training Loss=0.231]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 3/7:   0%|                               | 0/7 [00:27<?, ?it/s, Training Loss=0.178]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 3/7:   0%|                               | 0/7 [00:34<?, ?it/s, Training Loss=0.126]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 3/7:   0%|                                | 0/7 [00:41<?, ?it/s, Training Loss=0.08]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 3/7:   0%|                               | 0/7 [00:43<?, ?it/s, Training Loss=0.233]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7, Average Loss: 0.1984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Regression (MLM) Epoch 4/7:   0%|                                                    | 0/7 [00:00<?, ?it/s]\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BB33753320>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Regression (MLM) Epoch 4/7:   0%|                              | 0/7 [00:43<?, ?it/s, Training Loss=0.0497]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7, Average Loss: 0.1057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BB33753320>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-Tuning Regression (MLM) Epoch 5/7:   0%|                               | 0/7 [00:06<?, ?it/s, Training Loss=0.136]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 5/7:   0%|                              | 0/7 [00:13<?, ?it/s, Training Loss=0.0448]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 5/7:   0%|                              | 0/7 [00:20<?, ?it/s, Training Loss=0.0295]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 5/7:   0%|                              | 0/7 [00:27<?, ?it/s, Training Loss=0.0346]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 5/7:   0%|                               | 0/7 [00:34<?, ?it/s, Training Loss=0.039]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 5/7:   0%|                              | 0/7 [00:40<?, ?it/s, Training Loss=0.0755]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 5/7:   0%|                               | 0/7 [00:43<?, ?it/s, Training Loss=0.144]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7, Average Loss: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Regression (MLM) Epoch 6/7:   0%|                                                    | 0/7 [00:00<?, ?it/s]\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BB33753320>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Regression (MLM) Epoch 6/7:   0%|                               | 0/7 [00:43<?, ?it/s, Training Loss=0.031]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7, Average Loss: 0.0478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                                                       \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BB33753320>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-Tuning Regression (MLM) Epoch 7/7:   0%|                              | 0/7 [00:06<?, ?it/s, Training Loss=0.0325]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 7/7:   0%|                              | 0/7 [00:13<?, ?it/s, Training Loss=0.0501]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 7/7:   0%|                              | 0/7 [00:20<?, ?it/s, Training Loss=0.0466]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 7/7:   0%|                              | 0/7 [00:27<?, ?it/s, Training Loss=0.0556]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 7/7:   0%|                              | 0/7 [00:34<?, ?it/s, Training Loss=0.0474]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 7/7:   0%|                              | 0/7 [00:40<?, ?it/s, Training Loss=0.0276]\u001b[A\n",
      "Fine-Tuning Regression (MLM) Epoch 7/7:   0%|                              | 0/7 [00:43<?, ?it/s, Training Loss=0.0621]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7, Average Loss: 0.0460\n"
     ]
    }
   ],
   "source": [
    "# Define Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5  # Adjust if needed\n",
    "best_loss = float('inf')  # Initialize best loss to a large value\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(top_100_data_loader, desc=f\"Fine-Tuning Regression (MLM) Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    # print(top_100_data_loader)\n",
    "    for batch in top_100_data_loader:\n",
    "        # print(batch)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        # print(input_id)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "       \n",
    "       # Extract the predicted values corresponding to the true labels\n",
    "        outputs_reshaped = outputs.logits[:, 0] # Flatten logits\n",
    "        loss = criterion(outputs_reshaped, labels.float())  # Ensure labels are float for MSELoss\n",
    "\n",
    "        # loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"Training Loss\": loss.item()})\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(top_100_data_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9814c820-80be-4558-828f-53c9fb7da9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1bb634c9ca0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7eccc47e-e555-4a44-a05c-74153fc02743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | Eval Loss: 1.5043\n",
      "Best Evaluation Loss: 1.4986\n"
     ]
    }
   ],
   "source": [
    "# Define Loss and Optimizer for eval\n",
    "    # === EVALUATION PHASE ===\n",
    "model.eval()\n",
    "eval_loss = 0.0\n",
    "with torch.no_grad():  # Disable gradient calculations\n",
    "    for batch in test_data_loader:  # Use test data\n",
    "        # print(batch)\n",
    "        # input_ids = batch['input_ids'].to(device)\n",
    "        # attention_mask = batch['attention_mask'].to(device)\n",
    "        # labels = batch['label'].to(device)\n",
    "\n",
    "        smiles_list = batch[\"SMILES\"]  # Extract SMILES strings\n",
    "        labels = batch[\"label\"].to(torch.float).to(device)  # Convert labels to tensors\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            smiles_list,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,  # Adjust max_length as needed\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        outputs_reshaped = outputs.logits[:, 0] # Flatten logits\n",
    "        loss = criterion(outputs_reshaped, labels)\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "avg_eval_loss = eval_loss / len(test_data_loader)\n",
    "\n",
    "# Track Best Model\n",
    "if avg_eval_loss < best_loss:\n",
    "    best_loss = avg_eval_loss\n",
    "    torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "\n",
    "print(f\"Epoch {epoch+1}/{num_epochs} | Eval Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "print(f\"Best Evaluation Loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "851e6948-2dbd-4a86-95ff-62f1c931a3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | Eval Loss: 1.4986\n",
      "Best Evaluation Loss: 1.4986\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Epoch {epoch+1}/{num_epochs} | Eval Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "print(f\"Best Evaluation Loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60823c11-16b0-4ec0-979b-f5d1c6157ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
