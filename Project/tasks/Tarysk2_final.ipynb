{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03209e8-b7c4-47bd-b483-0de4607a100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from datasets import Dataset\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83905509-b16c-40d3-bd4a-17a474836cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and model paths\n",
    "DATASET_PATH = \"scikit-fingerprints/MoleculeNet_Lipophilicity\"\n",
    "MODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"\n",
    "EXTERNAL_DATASET_PATH = \"C:/Users/jadej/OneDrive/Desktop/Studies/NNTI 11th Feb/Project/Project_Files/tasks/External-Dataset_for_Task2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a7d65-cc67-4709-8cbb-ac5a7071b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b45a39-0b65-4392-a124-d71427e084d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from HuggingFace\n",
    "dataset = load_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2b0f1-11bf-4982-a82e-727cec03328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the maximum length of SMILES strings in the dataset\n",
    "max_length = max(len(smile) for smile in dataset['train']['SMILES'])\n",
    "print(f\"Max SMILES length: {max_length}\")\n",
    "\n",
    "\n",
    "\n",
    "# define a PyTorch Dataset class for handling SMILES strings and targets\n",
    "\n",
    "# TODO: your code goes here\n",
    "# class SMILESDataset(Dataset):\n",
    "\n",
    "\n",
    "class SMILEDATASET(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.rename(columns={'label': 'Label'})  # Standardize key names\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        return {\n",
    "            'SMILES': sample['SMILES'],\n",
    "            'Label': sample['Label']  # Ensure correct key mapping\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2b14d-8a4f-4e93-bf25-73543187c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external dataset\n",
    "ext_data = pd.read_csv(EXTERNAL_DATASET_PATH)\n",
    "# Load the pre-trained model and tokenizer\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# ext_data = ext_data1.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13316cb-79a3-43d3-9b10-32417dee6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d709f-3549-4a80-9a55-1fc9eb20447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, trust_remote_code=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4cc34-9b41-4ce6-bd44-90be486ddefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model, smiles, targets):\n",
    "    \"\"\"Compute gradients of loss with respect to model parameters.\"\"\"\n",
    "    model.zero_grad()\n",
    "\n",
    "    # ðŸ”¹ Ensure smiles is a string, not a tensor/dict\n",
    "    if isinstance(smiles, torch.Tensor):\n",
    "        smiles = smiles.item()  # Convert single tensor to string\n",
    "    elif isinstance(smiles, list) and isinstance(smiles[0], torch.Tensor):\n",
    "        smiles = [s.item() for s in smiles]  # Convert list of tensors to strings\n",
    "    \n",
    "    inputs = smiles\n",
    "    # ðŸ”¹ Ensure targets are in tensor format\n",
    "    # targets = torch.tensor([targets], dtype=torch.float32)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    \n",
    "    # ðŸ”¹ Forward pass\n",
    "    # outputs = model(**inputs)\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "\n",
    "    # ðŸ”¹ Compute loss\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    loss = loss_fn(outputs.logits.squeeze(), targets)\n",
    "\n",
    "    # ðŸ”¹ Compute gradients\n",
    "    loss.backward()\n",
    "    return [p.grad for p in model.parameters() if p.grad is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac979099-4f19-4867-bb04-2917fc282af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lissa_approximation(model, test_grad, train_data, iters=100, damping=0.01):\n",
    "    ihvp = [torch.zeros_like(tg) for tg in test_grad]\n",
    "\n",
    "    for _ in range(iters):\n",
    "        random_idx = random.randint(0, len(train_data) - 1)\n",
    "        train_sample = train_data[random_idx]  # Get one sample\n",
    "\n",
    "        # Debug print to check data\n",
    "        # print(f\"Sample retrieved: {train_sample}\")\n",
    "\n",
    "        smiles_input = train_sample.get('SMILES', None)\n",
    "        label_value = train_sample.get('label', None)  # Change from 'Label' to 'label'\n",
    "        # print(\"smiles lissa \", smiles_input)\n",
    "        inputs = tokenizer(train_sample['SMILES'], return_tensors=\"pt\")\n",
    "        label_value = torch.tensor([train_sample['label']])\n",
    "        # Safety check: If label or SMILES is missing, raise an error\n",
    "        if smiles_input is None or label_value is None:\n",
    "            raise KeyError(f\"Missing data in sample: {train_sample}\")\n",
    "\n",
    "        train_grad = compute_gradients(model, inputs, label_value)\n",
    "        ihvp = [g - damping * ih for g, ih in zip(train_grad, ihvp)]\n",
    "    \n",
    "    return ihvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302e9d3-5ff1-466e-a7cd-749d40eb4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_influence_scores(model, external_data, train_data):\n",
    "    \"\"\"Compute influence scores for each external data point.\"\"\"\n",
    "    influence_scores = []\n",
    "    for _, row in tqdm(external_data.iterrows(), total=len(external_data)):\n",
    "        # Tokenize input molecule SMILES string\n",
    "        inputs = tokenizer(row['SMILES'], return_tensors=\"pt\")\n",
    "        target = torch.tensor([row['Label']])\n",
    "        # print(\"done\")\n",
    "        \n",
    "        # Compute test gradient\n",
    "        test_grad = compute_gradients(model, inputs, target)\n",
    "\n",
    "        # Approximate inverse Hessian-vector product\n",
    "        ihvp = lissa_approximation(model, test_grad, train_data)\n",
    "\n",
    "        # Compute influence score (dot product of test gradient and ihvp)\n",
    "        influence = sum(torch.dot(tg.flatten(), ih.flatten()) for tg, ih in zip(test_grad, ihvp))\n",
    "        influence_scores.append((row['SMILES'], row['Label'], influence.item()))\n",
    "\n",
    "    return sorted(influence_scores, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac39ca7-23ac-4e1d-9ec0-ff502ff189cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training split of the Lipophilicity dataset (SMILEDATASET)\n",
    "SMILEDATASET = load_dataset(\"scikit-fingerprints/MoleculeNet_Lipophilicity\", split=\"train\")\n",
    "\n",
    "# Assign SMILEDATASET to train_data\n",
    "train_data = SMILEDATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c0ae02-1579-440c-9c43-2e7492e1d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute influence scores\n",
    "influence_scores = compute_influence_scores(model, ext_data, train_data)\n",
    "\n",
    "# Select top-48 influential samples\n",
    "selected_samples = influence_scores[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00409ec6-8eba-4fe7-ab3c-8628ed431943",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df = pd.DataFrame(influence_scores, columns=[\"Smiles\", \"Label\", \"Influence\"])\n",
    "inf_df.to_csv(\"Influence.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a287fd5-a04a-41ea-98d0-ede358da2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(influence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9118246-78ca-4f7f-a93f-d9f438a8bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788083d3-cd3d-4445-9625-2d9367c8fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "influence_df = pd.read_csv(\"Influence.csv\")  # Ensure the correct file path\n",
    "\n",
    "# Select the top 100 SMILES strings and corresponding Labels based on Influence score\n",
    "top_100 = influence_df.nlargest(100, 'Influence')[[\"Smiles\", \"Label\"]]  # Extract only required columns\n",
    "\n",
    "# Convert to lists\n",
    "top_100_smiles = top_100[\"Smiles\"].tolist()\n",
    "top_100_labels = top_100[\"Label\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "class TopInfluenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, smiles_list, labels_list, tokenizer, max_length=268):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.labels_list = labels_list  # Store labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length  # Set max length for consistent padding\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        label = torch.tensor(self.labels_list[idx], dtype=torch.float32)  # Convert label to tensor\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            smiles,\n",
    "            padding=\"max_length\",  # Ensure all sequences have the same length\n",
    "            truncation=True,  # Truncate if sequence exceeds max_length\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        tokenized_data = {key: val.squeeze(0) for key, val in encoded.items()}  # Remove batch dim\n",
    "        tokenized_data[\"label\"] = label  # Add label to dictionary\n",
    "\n",
    "        return tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "\n",
    "# Create dataset instance\n",
    "top_100_data_loader = torch.utils.data.DataLoader(\n",
    "    TopInfluenceDataset(top_100_smiles, top_100_labels, tokenizer),  # Pass only the SMILES list\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    # collate_fn=collate_fn  # Ensure proper batching\n",
    ")\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "# Split train_data into train (80%) and test (20%) sets\n",
    "train_size = int(0.8 * len(train_data))\n",
    "test_size = len(train_data) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(train_data, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for train and test data\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "# print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}, Top 100 dataset: {len(top_100_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d00cfa-c09c-41f3-ab37-01779749b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adca28f-5b71-4a28-abfb-8f36797eb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5  # Adjust if needed\n",
    "best_loss = float('inf')  # Initialize best loss to a large value\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(top_100_data_loader, desc=f\"Fine-Tuning Regression (MLM) Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    # print(top_100_data_loader)\n",
    "    for batch in top_100_data_loader:\n",
    "        # print(batch)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        # print(input_id)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "       \n",
    "       # Extract the predicted values corresponding to the true labels\n",
    "        outputs_reshaped = outputs.logits[:, 0] # Flatten logits\n",
    "        loss = criterion(outputs_reshaped, labels.float())  # Ensure labels are float for MSELoss\n",
    "\n",
    "        # loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"Training Loss\": loss.item()})\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(top_100_data_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9814c820-80be-4558-828f-53c9fb7da9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eccc47e-e555-4a44-a05c-74153fc02743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss and Optimizer for eval\n",
    "    # === EVALUATION PHASE ===\n",
    "model.eval()\n",
    "eval_loss = 0.0\n",
    "with torch.no_grad():  # Disable gradient calculations\n",
    "    for batch in test_data_loader:  # Use test data\n",
    "        # print(batch)\n",
    "        # input_ids = batch['input_ids'].to(device)\n",
    "        # attention_mask = batch['attention_mask'].to(device)\n",
    "        # labels = batch['label'].to(device)\n",
    "\n",
    "        smiles_list = batch[\"SMILES\"]  # Extract SMILES strings\n",
    "        labels = batch[\"label\"].to(torch.float).to(device)  # Convert labels to tensors\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            smiles_list,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,  # Adjust max_length as needed\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        outputs_reshaped = outputs.logits[:, 0] # Flatten logits\n",
    "        loss = criterion(outputs_reshaped, labels)\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "avg_eval_loss = eval_loss / len(test_data_loader)\n",
    "\n",
    "# Track Best Model\n",
    "if avg_eval_loss < best_loss:\n",
    "    best_loss = avg_eval_loss\n",
    "    torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "\n",
    "print(f\"Epoch {epoch+1}/{num_epochs} | Eval Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "print(f\"Best Evaluation Loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e6948-2dbd-4a86-95ff-62f1c931a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Epoch {epoch+1}/{num_epochs} | Eval Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "print(f\"Best Evaluation Loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60823c11-16b0-4ec0-979b-f5d1c6157ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
